{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shap\n",
    "# !pip install colorama\n",
    "\n",
    "from colorama import Fore, Style\n",
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "##################### Preprocessing imports \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "##################### Models\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#### Model selection \n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.optimize import minimize\n",
    "from IPython.display import clear_output\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "##################### optuna library import\n",
    "import optuna\n",
    "import shap\n",
    "random_state = 42\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Download the data \n",
    "train= pd.read_csv(\"../data/raw/train.csv\")\n",
    "test=pd.read_csv(\"../data/raw/test.csv\")\n",
    "sample = pd.read_csv('../data/raw/sample_submission.csv')\n",
    "data_dict=pd.read_csv(\"../data/raw/data_dictionary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Basic configuration </h3>\n",
    "<li>Map sii - map the target column in order to make the target data more clear</li>\n",
    "<li>perform basic actions - such as dropping the id column across the datasets and applying the map for the dataset</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Define Dependent and Target feature \n",
    "    target = 'sii'\n",
    "    Target_series = train['sii']\n",
    "    \n",
    "    # Drop the id column from all dataset respectively \n",
    "    train_id, test_id = train['id'], test['id']\n",
    "    train.drop(['id'], axis =1, inplace = True)\n",
    "    test.drop(['id'], axis =1, inplace = True)\n",
    "    data_dict = data_dict[data_dict['Field'].str.contains('id') == False]\n",
    "    \n",
    "except:\n",
    "    print(\"Already dropped id or name column\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [01:39<00:00, 10.04it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.73it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_file(filename, dirname):\n",
    "    '''\n",
    "    Reads a parquet file from a specific address using the pandas library into a dataframe\n",
    "    input: \n",
    "        filename - name of the file to read\n",
    "        dirname - name of the directory which the file is located\n",
    "    '''\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "\n",
    "######## Download the Tabular data \n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing action</h1>\n",
    "<p>In this section I am creating all the necassries function for the feature engineering part of the tabular dataset </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the dataframe with it's target series (also work when trying to combine it with other series)\n",
    "def concat_df_a_target(df,target):\n",
    "    return pd.concat([df,target], axis=1)\n",
    "\n",
    "def drop_Nans(train,subset):\n",
    "    dropped_df = train.dropna(subset=subset).reset_index().drop('index',axis=1)\n",
    "    return dropped_df\n",
    "\n",
    "###### Find the difference in features between two dataframes (mainly train and test)\n",
    "# Then return the new train set without it's unique column\n",
    "def feature_difference(train, test):\n",
    "    # Get the set of column names from each DataFrame\n",
    "    train_set, test_set = set(train.columns), set(test.columns)\n",
    "    # find the difference in cols\n",
    "    feature_diff = train_set - test_set\n",
    "\n",
    "    train = train.drop(list(feature_diff),axis=1)\n",
    "    train = concat_df_a_target(train, Target_series)    \n",
    "\n",
    "    return train\n",
    "\n",
    "\n",
    "###### Outliers handling, the following three function will be used to handle outliers values in the dataset. \n",
    "# First function is for capping outliers using an algorithms \n",
    "# Second function is for capping outliers using domain knowledge\n",
    "# Third function is to implement the process itself, which is combining the previous two functions\n",
    "def cap_outliers(train, columns, method='iqr', threshold=1.5):\n",
    "    '''\n",
    "    for the this tests I only want to remove outliers for some of the features and not all.\n",
    "    For example if I am not cartefull the function will alter the target column \n",
    "    because of that it is important to set to change only the desired columns\n",
    "    '''\n",
    "    \n",
    "    train_copy = train.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col != 'sii':\n",
    "            Q1 = train[col].quantile(0.25)\n",
    "            Q3 = train[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "            train_copy[col] = np.clip(train[col], lower_bound, upper_bound)\n",
    "        \n",
    "    return train_copy\n",
    "\n",
    "\n",
    "# This function is not neccasry\n",
    "def correct_outliers_dk(df):\n",
    "    train = df.copy()\n",
    "    # Define thresholds\n",
    "    bmi_threshold = 7\n",
    "    weight_threshold = 35\n",
    "    diastolic_bp_threshold = 35\n",
    "    systolic_bp_threshold = 65\n",
    "    heart_rate_threshold = 45\n",
    "\n",
    "    # Correct the outliers\n",
    "    train.loc[train['Physical-BMI'] <= bmi_threshold, 'Physical-BMI'] = bmi_threshold\n",
    "    train.loc[train['Physical-Weight'] <= weight_threshold, 'Physical-Weight'] = weight_threshold\n",
    "    train.loc[train['Physical-Diastolic_BP'] < diastolic_bp_threshold, 'Physical-Diastolic_BP'] = diastolic_bp_threshold\n",
    "    train.loc[train['Physical-Systolic_BP'] < systolic_bp_threshold, 'Physical-Systolic_BP'] = systolic_bp_threshold\n",
    "    train.loc[train['Physical-HeartRate'] < heart_rate_threshold, 'Physical-HeartRate'] = heart_rate_threshold\n",
    "    swap_condition = train['Physical-Diastolic_BP'] > train['Physical-Systolic_BP']\n",
    "    train.loc[swap_condition, ['Physical-Diastolic_BP', 'Physical-Systolic_BP']] = train.loc[swap_condition, ['Physical-Systolic_BP', 'Physical-Diastolic_BP']].values\n",
    "    \n",
    "    return train\n",
    "\n",
    "\n",
    "def handle_outliers(train): \n",
    "    '''\n",
    "    The following function handle the outliers, both the statistical domain knowledge. \n",
    "    The function receives from the user \n",
    "    train - the train dataframe set\n",
    "    '''\n",
    "    \n",
    "    train_capper = cap_outliers(train,train.select_dtypes(include='number').columns)\n",
    "    display(train_capper.describe())\n",
    "    \n",
    "    return train_capper\n",
    "\n",
    "###### Find high correlation pairs (pairs of features that have a correlation of over the threshold which currently stands on 95%)\n",
    "# later on those features will be reduced in order to clean the data\n",
    "def high_correlation_pairs(train, threshold=0.95):\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = train.select_dtypes(include='number').corr(\"pearson\")\n",
    "    \n",
    "    # Select pairs of features with correlations above the threshold\n",
    "    high_corr_pairs = (\n",
    "        corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))  # Upper triangle without diagonal\n",
    "        .stack()  # Convert to Series\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Rename columns for readability\n",
    "    high_corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "    \n",
    "    # Filter by the correlation threshold (both positive and negative)\n",
    "    high_corr_pairs = high_corr_pairs[high_corr_pairs['Correlation'].abs() > threshold]\n",
    "    \n",
    "    # Display the high correlation pairs\n",
    "    print(\"Highly correlated feature pairs (|correlation| > {}):\".format(threshold))\n",
    "    print(high_corr_pairs.to_string(index=False))\n",
    "    \n",
    "    return high_corr_pairs\n",
    "\n",
    "\n",
    "def drop_high_cor_pairs(train,test):\n",
    "    high_corr_pairs = high_correlation_pairs(train)\n",
    "    # take the second feature from each pair and drop them from the dataframe\n",
    "    features_to_remove = high_corr_pairs['Feature 2'].tolist()\n",
    "    train = train.drop(features_to_remove, axis=1)\n",
    "    test = test.drop(features_to_remove, axis=1)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "# Find features with less then threshold correlation\n",
    "def low_correlated_features(train, target_column, threshold=0.1):\n",
    "\n",
    "    corr_matrix = train.corr()\n",
    "    target_correlations = corr_matrix[target_column].abs()\n",
    "    low_correlated_features = target_correlations[target_correlations < threshold].index\n",
    "    return low_correlated_features\n",
    "\n",
    "\n",
    "def imputing_missing_data(train, test):\n",
    "    train_without_target = train.drop('sii', axis=1)\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    train_imputed = imputer.fit_transform(train_without_target)\n",
    "    test_imputed = imputer.transform(test)\n",
    "\n",
    "    # Imputation converts the dataframe type to a numpy, therefore there is a need to restore that to its original data type\n",
    "    train_imputed = pd.DataFrame(train_imputed, columns=train_without_target.columns)\n",
    "    train_imputed = concat_df_a_target(train_imputed,Target_series)\n",
    "    test_imputed = pd.DataFrame(test_imputed, columns=train_without_target.columns)\n",
    "    return train_imputed, test_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df,tst):\n",
    "    '''\n",
    "    This function is used to clean the data and make sure it is ready for modeling.\n",
    "    The function will only work on numerical features as the categorical will be studied separately and later on will unite\n",
    "    The function works on a copy so it won't change the original training set and accidently cause data leakage\n",
    "    input:\n",
    "        df: the test dataframe\n",
    "        tst: the train dataframe\n",
    "    output:\n",
    "        The processed train and test sets ready for modeling \n",
    "    '''\n",
    "    \n",
    "    train, test = df.copy(), tst.copy()\n",
    "    ### Drop categorical columns \n",
    "    cat_cols = train.select_dtypes(exclude='number').columns\n",
    "    train = train.drop(cat_cols,axis=1)\n",
    "    cat_cols = test.select_dtypes(exclude='number').columns\n",
    "    test = test.drop(cat_cols,axis=1)\n",
    "\n",
    "    ### Drop unique columns for train dataframe\n",
    "    train = feature_difference(train, test) \n",
    "    \n",
    "    ### Deal with outliers\n",
    "    train = handle_outliers(train)\n",
    "\n",
    "    ### Handle missing values\n",
    "    train, test = imputing_missing_data(train,test)\n",
    "    train = train.dropna(subset='sii').reset_index().drop('index',axis=1)\n",
    "\n",
    "    ### Feature creation \n",
    "    #feature_creation(train,test)\n",
    "\n",
    "    ### Drop high correlation pairs\n",
    "    train, test = drop_high_cor_pairs(train,test)\n",
    "    \n",
    "    ### Drop features with low correlation to target\n",
    "    low_corr_cols = low_correlated_features(train.select_dtypes(include='number'),'sii')\n",
    "    train, test = train.drop(low_corr_cols,axis=1), test.drop(low_corr_cols,axis=1)\n",
    "\n",
    "    return train, test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Basic_Demos-Age</th>\n",
       "      <th>Basic_Demos-Sex</th>\n",
       "      <th>CGAS-CGAS_Score</th>\n",
       "      <th>Physical-BMI</th>\n",
       "      <th>Physical-Height</th>\n",
       "      <th>Physical-Weight</th>\n",
       "      <th>Physical-Waist_Circumference</th>\n",
       "      <th>Physical-Diastolic_BP</th>\n",
       "      <th>Physical-HeartRate</th>\n",
       "      <th>Physical-Systolic_BP</th>\n",
       "      <th>...</th>\n",
       "      <th>BIA-BIA_LDM</th>\n",
       "      <th>BIA-BIA_LST</th>\n",
       "      <th>BIA-BIA_SMM</th>\n",
       "      <th>BIA-BIA_TBW</th>\n",
       "      <th>PAQ_A-PAQ_A_Total</th>\n",
       "      <th>PAQ_C-PAQ_C_Total</th>\n",
       "      <th>SDS-SDS_Total_Raw</th>\n",
       "      <th>SDS-SDS_Total_T</th>\n",
       "      <th>PreInt_EduHx-computerinternet_hoursday</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3960.000000</td>\n",
       "      <td>3960.000000</td>\n",
       "      <td>2421.000000</td>\n",
       "      <td>3022.000000</td>\n",
       "      <td>3027.000000</td>\n",
       "      <td>3076.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>2954.000000</td>\n",
       "      <td>2967.000000</td>\n",
       "      <td>2954.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1991.000000</td>\n",
       "      <td>1991.000000</td>\n",
       "      <td>1991.000000</td>\n",
       "      <td>1991.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>1721.000000</td>\n",
       "      <td>2609.000000</td>\n",
       "      <td>2606.000000</td>\n",
       "      <td>3301.000000</td>\n",
       "      <td>2736.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.426894</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>65.093350</td>\n",
       "      <td>19.144650</td>\n",
       "      <td>55.946713</td>\n",
       "      <td>88.136749</td>\n",
       "      <td>27.151002</td>\n",
       "      <td>69.217840</td>\n",
       "      <td>81.568082</td>\n",
       "      <td>116.535206</td>\n",
       "      <td>...</td>\n",
       "      <td>18.017398</td>\n",
       "      <td>63.499017</td>\n",
       "      <td>31.179007</td>\n",
       "      <td>49.948860</td>\n",
       "      <td>2.178853</td>\n",
       "      <td>2.589550</td>\n",
       "      <td>40.875623</td>\n",
       "      <td>57.577322</td>\n",
       "      <td>1.060588</td>\n",
       "      <td>0.580409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.554845</td>\n",
       "      <td>0.483591</td>\n",
       "      <td>11.775849</td>\n",
       "      <td>4.379203</td>\n",
       "      <td>7.473764</td>\n",
       "      <td>41.682765</td>\n",
       "      <td>5.176501</td>\n",
       "      <td>11.909234</td>\n",
       "      <td>13.468212</td>\n",
       "      <td>15.052733</td>\n",
       "      <td>...</td>\n",
       "      <td>6.786950</td>\n",
       "      <td>24.111157</td>\n",
       "      <td>13.107502</td>\n",
       "      <td>18.450158</td>\n",
       "      <td>0.849476</td>\n",
       "      <td>0.783937</td>\n",
       "      <td>9.766982</td>\n",
       "      <td>12.674429</td>\n",
       "      <td>1.094875</td>\n",
       "      <td>0.771122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>7.316511</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.635810</td>\n",
       "      <td>23.620100</td>\n",
       "      <td>4.655730</td>\n",
       "      <td>20.589200</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>15.869350</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>57.200000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.983150</td>\n",
       "      <td>45.204100</td>\n",
       "      <td>21.141550</td>\n",
       "      <td>35.887000</td>\n",
       "      <td>1.490000</td>\n",
       "      <td>2.020000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>17.937682</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.438800</td>\n",
       "      <td>56.996400</td>\n",
       "      <td>27.415100</td>\n",
       "      <td>44.987000</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>2.540000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>21.571244</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>113.800000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>90.500000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>22.167600</td>\n",
       "      <td>77.105650</td>\n",
       "      <td>38.179400</td>\n",
       "      <td>60.271050</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>3.160000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>30.124083</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>198.700000</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>98.500000</td>\n",
       "      <td>118.250000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>35.944275</td>\n",
       "      <td>124.957975</td>\n",
       "      <td>63.736175</td>\n",
       "      <td>96.847125</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.790000</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>89.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Basic_Demos-Age  Basic_Demos-Sex  CGAS-CGAS_Score  Physical-BMI  \\\n",
       "count      3960.000000      3960.000000      2421.000000   3022.000000   \n",
       "mean         10.426894         0.372727        65.093350     19.144650   \n",
       "std           3.554845         0.483591        11.775849      4.379203   \n",
       "min           5.000000         0.000000        35.000000      7.316511   \n",
       "25%           8.000000         0.000000        59.000000     15.869350   \n",
       "50%          10.000000         0.000000        65.000000     17.937682   \n",
       "75%          13.000000         1.000000        75.000000     21.571244   \n",
       "max          20.500000         1.000000        99.000000     30.124083   \n",
       "\n",
       "       Physical-Height  Physical-Weight  Physical-Waist_Circumference  \\\n",
       "count      3027.000000      3076.000000                    898.000000   \n",
       "mean         55.946713        88.136749                     27.151002   \n",
       "std           7.473764        41.682765                      5.176501   \n",
       "min          33.000000         0.000000                     18.000000   \n",
       "25%          50.000000        57.200000                     23.000000   \n",
       "50%          55.000000        77.000000                     26.000000   \n",
       "75%          62.000000       113.800000                     30.000000   \n",
       "max          78.500000       198.700000                     40.500000   \n",
       "\n",
       "       Physical-Diastolic_BP  Physical-HeartRate  Physical-Systolic_BP  ...  \\\n",
       "count            2954.000000         2967.000000           2954.000000  ...   \n",
       "mean               69.217840           81.568082            116.535206  ...   \n",
       "std                11.909234           13.468212             15.052733  ...   \n",
       "min                38.500000           44.250000             80.000000  ...   \n",
       "25%                61.000000           72.000000            107.000000  ...   \n",
       "50%                68.000000           81.000000            114.000000  ...   \n",
       "75%                76.000000           90.500000            125.000000  ...   \n",
       "max                98.500000          118.250000            152.000000  ...   \n",
       "\n",
       "       BIA-BIA_LDM  BIA-BIA_LST  BIA-BIA_SMM  BIA-BIA_TBW  PAQ_A-PAQ_A_Total  \\\n",
       "count  1991.000000  1991.000000  1991.000000  1991.000000         475.000000   \n",
       "mean     18.017398    63.499017    31.179007    49.948860           2.178853   \n",
       "std       6.786950    24.111157    13.107502    18.450158           0.849476   \n",
       "min       4.635810    23.620100     4.655730    20.589200           0.660000   \n",
       "25%      12.983150    45.204100    21.141550    35.887000           1.490000   \n",
       "50%      16.438800    56.996400    27.415100    44.987000           2.010000   \n",
       "75%      22.167600    77.105650    38.179400    60.271050           2.780000   \n",
       "max      35.944275   124.957975    63.736175    96.847125           4.710000   \n",
       "\n",
       "       PAQ_C-PAQ_C_Total  SDS-SDS_Total_Raw  SDS-SDS_Total_T  \\\n",
       "count        1721.000000        2609.000000      2606.000000   \n",
       "mean            2.589550          40.875623        57.577322   \n",
       "std             0.783937           9.766982        12.674429   \n",
       "min             0.580000          17.000000        38.000000   \n",
       "25%             2.020000          33.000000        47.000000   \n",
       "50%             2.540000          39.000000        55.000000   \n",
       "75%             3.160000          46.000000        64.000000   \n",
       "max             4.790000          65.500000        89.500000   \n",
       "\n",
       "       PreInt_EduHx-computerinternet_hoursday          sii  \n",
       "count                             3301.000000  2736.000000  \n",
       "mean                                 1.060588     0.580409  \n",
       "std                                  1.094875     0.771122  \n",
       "min                                  0.000000     0.000000  \n",
       "25%                                  0.000000     0.000000  \n",
       "50%                                  1.000000     0.000000  \n",
       "75%                                  2.000000     1.000000  \n",
       "max                                  3.000000     3.000000  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated feature pairs (|correlation| > 0.95):\n",
      "                  Feature 1                   Feature 2  Correlation\n",
      "Fitness_Endurance-Max_Stage Fitness_Endurance-Time_Mins     0.961850\n",
      "                BIA-BIA_BMR                 BIA-BIA_ECW     0.970422\n",
      "                BIA-BIA_BMR                 BIA-BIA_FFM     1.000000\n",
      "                BIA-BIA_BMR                 BIA-BIA_ICW     0.971878\n",
      "                BIA-BIA_BMR                 BIA-BIA_LDM     0.954621\n",
      "                BIA-BIA_BMR                 BIA-BIA_LST     0.996213\n",
      "                BIA-BIA_BMR                 BIA-BIA_SMM     0.966130\n",
      "                BIA-BIA_BMR                 BIA-BIA_TBW     0.993596\n",
      "                BIA-BIA_ECW                 BIA-BIA_FFM     0.970422\n",
      "                BIA-BIA_ECW                 BIA-BIA_LST     0.970483\n",
      "                BIA-BIA_ECW                 BIA-BIA_TBW     0.978773\n",
      "                BIA-BIA_FFM                 BIA-BIA_ICW     0.971878\n",
      "                BIA-BIA_FFM                 BIA-BIA_LDM     0.954621\n",
      "                BIA-BIA_FFM                 BIA-BIA_LST     0.996213\n",
      "                BIA-BIA_FFM                 BIA-BIA_SMM     0.966130\n",
      "                BIA-BIA_FFM                 BIA-BIA_TBW     0.993596\n",
      "                BIA-BIA_ICW                 BIA-BIA_LST     0.968760\n",
      "                BIA-BIA_ICW                 BIA-BIA_SMM     0.959168\n",
      "                BIA-BIA_ICW                 BIA-BIA_TBW     0.975828\n",
      "                BIA-BIA_LST                 BIA-BIA_SMM     0.962297\n",
      "                BIA-BIA_LST                 BIA-BIA_TBW     0.992055\n",
      "                BIA-BIA_SMM                 BIA-BIA_TBW     0.961141\n",
      "          SDS-SDS_Total_Raw             SDS-SDS_Total_T     0.998387\n"
     ]
    }
   ],
   "source": [
    "train_processed,test_processed = feature_engineering(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Combine sets - Numerical, Categorical and Timeseries summary</h2>\n",
    "Combine the categrocial features dataset with the numerical features dataset\n",
    "</br>\n",
    "First I combine the numerical dataset I got from after performing preprocessing with the categorical features that I choose based on different metrics.\n",
    "</br>\n",
    "After that I take the new dataframe and add to it all the features from the Timeseries summary dataframe that I have created earlier using the parquet timeseries files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the best categorical feature based on correlation to the target feature \n",
    "best_categorical_features = [\"SDS-Season_Spring\",\"SDS-Season_Winter\",\"SDS-Season_Fall\",\"CGAS-Season_Spring\",\"SDS-Season_Summer\",\n",
    "                             \"CGAS-Season_Winter\",\"Fitness_Endurance-Season_Spring\",\"CGAS-Season_Fall\",\"BIA-Season_Fall\",\"id\"]\n",
    "\n",
    "# Convert dataframes to dummies \n",
    "train_dummies = pd.get_dummies(train)\n",
    "test_dummies = pd.get_dummies(test)\n",
    "train_dummies['id'],test_dummies['id'] = train_id, test_id\n",
    "\n",
    "# keep only the column that were defined in the beggining \n",
    "train_dummies = train_dummies.dropna(subset=\"sii\").reset_index().drop('index',axis=1)\n",
    "train_cat_best = train_dummies[best_categorical_features]\n",
    "test_cat_best = test_dummies[best_categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat between the categorical dataframe and the numerical dataframe \n",
    "train = pd.concat([train_processed, train_cat_best], axis=1, ignore_index=False)\n",
    "test = pd.concat([test_processed, test_cat_best], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat between tabular data and timeseries summary dataframe\n",
    "try:\n",
    "    train = train.merge(train_ts, on='id', how='left')\n",
    "    train.drop(['id'], axis =1, inplace = True)\n",
    "    train['accelerometer_data'] = train['stat_0'].notnull()\n",
    "    \n",
    "    test = test.merge(test_ts, on='id', how='left')\n",
    "    test.drop(['id'], axis =1, inplace = True)\n",
    "    test['accelerometer_data'] = test['stat_0'].notnull()\n",
    "\n",
    "except:\n",
    "    print(\"Already dropped id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Evaluation </h1>\n",
    "Evaluate the model using the QWK metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [01:31<00:00, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.9638\n",
      "Mean Validation QWK ---> 0.3723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.438\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    2\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    1\n",
       "4   0016bb22    2\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    2\n",
       "9   0083e397    2\n",
       "10  0087dd65    0\n",
       "11  00abe655    0\n",
       "12  00ae59c9    2\n",
       "13  00af6387    2\n",
       "14  00bd4359    2\n",
       "15  00c0cd71    2\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Light = LGBMRegressor(verbose = -1)\n",
    "XGB_Model = XGBRegressor()\n",
    "CatBoost_Model = CatBoostRegressor(verbose = False)\n",
    "\n",
    "\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model)\n",
    "])\n",
    "\n",
    "Submission = TrainML(voting_model, test)\n",
    "\n",
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
