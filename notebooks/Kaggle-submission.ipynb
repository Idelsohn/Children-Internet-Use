{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install skimpy --quiet\n# !pip install wordcloud --quiet\n# !pip install category_encoders --quiet\n# !pip install shap\n!pip install colorama\nfrom colorama import Fore, Style\n# Basic imports\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport pandas as pd\n#import skimpy\nimport re\nimport time\nimport random\nimport datetime\nimport warnings\nfrom tqdm import tqdm\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", message=\"Parameters: { 'verbose' } are not used.\")\n\n##################### Preprocessing imports \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom category_encoders import TargetEncoder\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom scipy.optimize import minimize\nfrom IPython.display import clear_output\n\n##################### Models\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\n##################### optuna library import\nimport optuna\nimport shap\nrandom_state = 42\nn_splits = 5\n\n#!pip install opendatasetS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:29.287646Z","iopub.execute_input":"2024-12-05T15:08:29.288581Z","iopub.status.idle":"2024-12-05T15:09:11.780966Z","shell.execute_reply.started":"2024-12-05T15:08:29.288513Z","shell.execute_reply":"2024-12-05T15:09:11.779422Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (0.4.6)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"######## Download the data \ntrain=pd.read_csv(\"/kaggle/input/child-mind-institute-problematic-internet-use/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/child-mind-institute-problematic-internet-use/test.csv\")\ndata_dict=pd.read_csv(\"/kaggle/input/child-mind-institute-problematic-internet-use/data_dictionary.csv\")\nsample=pd.read_csv(\"/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:22.813406Z","iopub.status.idle":"2024-12-05T15:08:22.813784Z","shell.execute_reply.started":"2024-12-05T15:08:22.813609Z","shell.execute_reply":"2024-12-05T15:08:22.813628Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h3> Basic configuration </h3>\n<li>Map sii - map the target column in order to make the target data more clear</li>\n<li>perform basic actions - such as dropping the id column across the datasets and applying the map for the dataset</li>","metadata":{}},{"cell_type":"code","source":"try:\n    # Define Dependent and Target feature \n    target = 'sii'\n    Target_series = train['sii']\n    \n    # Drop the id column from all dataset respectively \n    train.drop(['id'], axis =1, inplace = True)\n    test.drop(['id'], axis =1, inplace = True)\n    data_dict = data_dict[data_dict['Field'].str.contains('id') == False]\n    \n    # Create a copy \n    train_copy = train.copy()\n \nexcept:\n    print(\"Already dropped id or name column\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:22.815213Z","iopub.status.idle":"2024-12-05T15:08:22.815668Z","shell.execute_reply.started":"2024-12-05T15:08:22.815459Z","shell.execute_reply":"2024-12-05T15:08:22.815491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h3>SelectKBest</h3>\nFind best categorical features using SelectKBest. This function selects the feature with the higest chi-squared score relative to the target column ","metadata":{}},{"cell_type":"code","source":"def Kbest(cat_dummies):\n    # Ten features with highest chi-squared statistics are selected\n    chi2_features = SelectKBest(chi2, k=10)\n    X_kbest_features = chi2_features.fit_transform(cat_dummies.drop(\"sii\",axis=1),cat_dummies[\"sii\"])\n    \n    feature_names = cat_dummies.drop(\"sii\", axis=1).columns\n    \n    scores = chi2_features.scores_\n    \n    # Create a DataFrame to visualize the results\n    feature_scores = pd.DataFrame({'Feature': feature_names, 'Score': scores})\n    \n    # Sort the DataFrame by scores in descending order\n    feature_scores = feature_scores.sort_values(by='Score', ascending=False)\n    \n    # Print the top 10 features\n    print(feature_scores.head(10))\n    # Reduced features\n    print('Original feature number:', cat_dummies.shape[1])\n    print('Reduced feature number:', X_kbest_features.shape[1])\n\n    # Return the dataframe with only the columns that have the highest chi2 square correlation\n    return cat_dummies[list(feature_scores[0:10].Feature)].reset_index().drop('index',axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:22.818855Z","iopub.status.idle":"2024-12-05T15:08:22.819244Z","shell.execute_reply.started":"2024-12-05T15:08:22.819067Z","shell.execute_reply":"2024-12-05T15:08:22.819088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### Combines dataframe with target series\ndef concat_df_a_target(df,target):\n    return pd.concat([df,target], axis=1)\n\ndef drop_Nans(train,subset):\n    dropped_df = train.dropna(subset=subset).reset_index().drop('index',axis=1)\n    return dropped_df\n\n\n# Creates a dataframe of the categorical features converted to dummies\ndef cat_of_dummies(train):\n    # Create categorical dataframe consist only categorical features\n    train_cat = train.select_dtypes(exclude='number')\n    # Make a categorical dataframe of dummies\n    train_cat_dum= pd.get_dummies(train_cat)\n\n    return train_cat_dum\n\ndef process_cat_of_dummies(train,test):\n    '''\n    This function is used for processing the dataframe of categorical dummies\n    '''\n    # Create a train and a test sets containing only categorical\n    train_cat = cat_of_dummies(train)\n    test_cat = cat_of_dummies(test)\n    \n    # Drop uncommon columns from train (this will leave only the columns that appear in both the train and test)\n    common_cols = list(set(train_cat.columns).intersection(set(test_cat.columns)))\n    train_cat = train_cat[common_cols]\n    \n    train_cat = concat_df_a_target(train_cat,Target_series)\n    train_cat = drop_Nans(train_cat,\"sii\")\n\n    return train_cat, test_cat\n\ntrain_cat, test_cat = process_cat_of_dummies(train, test) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:22.820494Z","iopub.status.idle":"2024-12-05T15:08:22.820964Z","shell.execute_reply.started":"2024-12-05T15:08:22.820752Z","shell.execute_reply":"2024-12-05T15:08:22.820781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_cat_best = Kbest(train_cat)\n# Define the test categorical set which includes all the categorical columns from the train\ntest_cat_best = test_cat[train_cat_best.columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:22.823341Z","iopub.status.idle":"2024-12-05T15:08:22.823870Z","shell.execute_reply.started":"2024-12-05T15:08:22.823591Z","shell.execute_reply":"2024-12-05T15:08:22.823619Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1>Preprocessing action</h1>\n<p>In this section I am creating all the necassries function for the feature engineering part of the tabular dataset </p>","metadata":{}},{"cell_type":"code","source":"def feature_difference(train, test):\n    # Get the set of column names from each DataFrame\n    train_set = set(train.columns)\n    test_set = set(test.columns)\n\n    # find the difference in cols\n    feature_difference_cols = train_set - test_set\n\n    return feature_difference_cols\n\n\ndef cap_outliers(train, columns, method='iqr', threshold=1.5):\n    '''\n    for the this tests I only want to remove outliers for some of the features and not all.\n    For example if I am not cartefull the function will alter the target column \n    because of that it is important to set to change only the desired columns\n    '''\n    \n    train_copy = train.copy()\n    \n    for col in columns:\n        if col != 'sii':\n            Q1 = train[col].quantile(0.25)\n            Q3 = train[col].quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - threshold * IQR\n            upper_bound = Q3 + threshold * IQR\n\n            train_copy[col] = np.clip(train[col], lower_bound, upper_bound)\n        \n    return train_copy\n\n\n# This function is not neccasry\ndef correct_outliers_dk(df):\n    train = df.copy()\n    # Define thresholds\n    bmi_threshold = 7\n    weight_threshold = 35\n    diastolic_bp_threshold = 35\n    systolic_bp_threshold = 65\n    heart_rate_threshold = 45\n\n    # Correct the outliers\n    train.loc[train['Physical-BMI'] <= bmi_threshold, 'Physical-BMI'] = bmi_threshold\n    train.loc[train['Physical-Weight'] <= weight_threshold, 'Physical-Weight'] = weight_threshold\n    train.loc[train['Physical-Diastolic_BP'] < diastolic_bp_threshold, 'Physical-Diastolic_BP'] = diastolic_bp_threshold\n    train.loc[train['Physical-Systolic_BP'] < systolic_bp_threshold, 'Physical-Systolic_BP'] = systolic_bp_threshold\n    train.loc[train['Physical-HeartRate'] < heart_rate_threshold, 'Physical-HeartRate'] = heart_rate_threshold\n    swap_condition = train['Physical-Diastolic_BP'] > train['Physical-Systolic_BP']\n    train.loc[swap_condition, ['Physical-Diastolic_BP', 'Physical-Systolic_BP']] = train.loc[swap_condition, ['Physical-Systolic_BP', 'Physical-Diastolic_BP']].values\n    \n    return train\n\ndef handle_outliers(train): \n    '''\n    The following function handle the outliers, both the statistical domain knowledge. \n    The function receives from the user \n    train - the train dataframe set\n    '''\n    \n    train_capper = cap_outliers(train,train.select_dtypes(include='number').columns)\n    display(train_capper.describe())\n    \n    return train_capper\n\n\ndef high_correlation_pairs(train, threshold=0.95):\n    # Calculate the correlation matrix\n    corr_matrix = train.select_dtypes(include='number').corr(\"pearson\")\n    \n    # Select pairs of features with correlations above the threshold\n    high_corr_pairs = (\n        corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))  # Upper triangle without diagonal\n        .stack()  # Convert to Series\n        .reset_index()\n    )\n    \n    # Rename columns for readability\n    high_corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']\n    \n    # Filter by the correlation threshold (both positive and negative)\n    high_corr_pairs = high_corr_pairs[high_corr_pairs['Correlation'].abs() > threshold]\n    \n    # Display the high correlation pairs\n    print(\"Highly correlated feature pairs (|correlation| > {}):\".format(threshold))\n    print(high_corr_pairs.to_string(index=False))\n    \n    return high_corr_pairs\n\n\n# Find features with less then threshold correlation\ndef low_correlated_features(train, target_column, threshold=0.1):\n\n    corr_matrix = train.corr()\n    target_correlations = corr_matrix[target_column].abs()\n    low_correlated_features = target_correlations[target_correlations < threshold].index\n    \n    return low_correlated_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:22.825570Z","iopub.status.idle":"2024-12-05T15:08:22.826125Z","shell.execute_reply.started":"2024-12-05T15:08:22.825829Z","shell.execute_reply":"2024-12-05T15:08:22.825858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_engineering(df,tst):\n    '''\n        This function is used to clean the data and make sure it is ready for modeling\n        input:\n            train: the test dataframe\n            test: the train dataframe           \n    '''\n    \n    train = df.copy()\n    test= tst.copy()\n    ### Drop unique columns for train dataframe (later add the target series as it was dropped)\n    cols_diff = feature_difference(train, test) \n    train = train.drop(list(cols_diff),axis=1)\n    train = concat_df_a_target(train, Target_series)\n    \n    display(train)\n    ### Feature creation \n    #feature_creation(train,test)\n    \n    ### Handle missing values\n    train = train.dropna(subset='sii').reset_index().drop('index',axis=1)\n    \n    ### Drop high correlation pairs\n    high_corr_pairs = high_correlation_pairs(train)\n    # take the second feature from each pair and drop them from the dataframe\n    features_to_remove = high_corr_pairs['Feature 2'].tolist()\n    train = train.drop(features_to_remove, axis=1)\n    test = test.drop(features_to_remove, axis=1)\n\n    ### Drop features with low correlation to target\n    low_corr_cols = low_correlated_features(train.select_dtypes(include='number'),'sii')\n    train = train.drop(low_corr_cols,axis=1)\n    test = test.drop(low_corr_cols,axis=1)\n\n    ### Deal with outliers\n    train = handle_outliers(train)\n\n    ### Drop categorical columns \n    cat_cols = train.select_dtypes(exclude='number').columns\n    train = train.drop(cat_cols,axis=1)\n    test = test.drop(cat_cols,axis=1)\n    \n    return train, test ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:22.828199Z","iopub.status.idle":"2024-12-05T15:08:22.828747Z","shell.execute_reply.started":"2024-12-05T15:08:22.828462Z","shell.execute_reply":"2024-12-05T15:08:22.828491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_processed,test_processed = feature_engineering(train,test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:22.830175Z","iopub.status.idle":"2024-12-05T15:08:22.830770Z","shell.execute_reply.started":"2024-12-05T15:08:22.830427Z","shell.execute_reply":"2024-12-05T15:08:22.830455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>Combine sets</h2>\nCombine the categrocial features dataset with the numerical features dataset","metadata":{}},{"cell_type":"code","source":"# Concat between the categorical dataframe and the numerical dataframe \ntrain = pd.concat([train_processed, train_cat_best], axis=1, ignore_index=False)\ntest = pd.concat([test_processed, test_cat_best], axis=1, ignore_index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:22.832488Z","iopub.status.idle":"2024-12-05T15:08:22.833046Z","shell.execute_reply.started":"2024-12-05T15:08:22.832736Z","shell.execute_reply":"2024-12-05T15:08:22.832763Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1> Evaluation </h1>\nEvaluate the model using the QWK metric ","metadata":{}},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\n\ndef TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:22.835793Z","iopub.status.idle":"2024-12-05T15:08:22.836478Z","shell.execute_reply.started":"2024-12-05T15:08:22.836177Z","shell.execute_reply":"2024-12-05T15:08:22.836208Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Light = LGBMRegressor(verbose = -1)\nXGB_Model = XGBRegressor()\nCatBoost_Model = CatBoostRegressor(verbose = False)\n\n\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n])\n\nSubmission = TrainML(voting_model, test)\n\nSubmission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:22.837696Z","iopub.status.idle":"2024-12-05T15:08:22.838281Z","shell.execute_reply.started":"2024-12-05T15:08:22.837983Z","shell.execute_reply":"2024-12-05T15:08:22.838030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Submission.to_csv(\"submission.csv\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:08:22.839533Z","iopub.status.idle":"2024-12-05T15:08:22.840173Z","shell.execute_reply.started":"2024-12-05T15:08:22.839871Z","shell.execute_reply":"2024-12-05T15:08:22.839902Z"}},"outputs":[],"execution_count":null}]}