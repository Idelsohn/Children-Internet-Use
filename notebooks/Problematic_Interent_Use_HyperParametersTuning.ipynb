{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section id=\"sec1\"> </section>\n",
    "<h1> Imports </h1>\n",
    "\n",
    "<a href=\"#back\" style=\"text-decoration: none; color: #333;\">Back to table of contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nadav\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Parameters: { 'verbose' } are not used.\")\n",
    "\n",
    "##################### sklearn imports \n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.base import clone\n",
    "\n",
    "##################### Models\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "##################### optuna library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\nadav\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from optuna) (1.13.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from optuna) (6.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from optuna) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from optuna) (23.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from optuna) (1.4.39)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\nadav\\anaconda3\\lib\\site-packages (0.46.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from shap) (1.24.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from shap) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from shap) (1.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from shap) (4.65.0)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from shap) (23.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from shap) (0.57.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from numba->shap) (0.40.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from pandas->shap) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from pandas->shap) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nadav\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna\n",
    "!pip install shap\n",
    "import optuna\n",
    "import shap\n",
    "random_state = 42\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m######## Download the data \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#train=pd.read_csv(\"train_preprocessed.csv\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#test=pd.read_csv(\"test_preprocessed.csv\")\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/processed/train_processed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m test\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/processed/test_processed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m sample \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/sample_submission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "######## Download the data \n",
    "#train=pd.read_csv(\"train_preprocessed.csv\")\n",
    "#test=pd.read_csv(\"test_preprocessed.csv\")\n",
    "\n",
    "train=pd.read_csv(\"../data/processed/train_processed.csv\")\n",
    "test=pd.read_csv(\"../data/processed/test_processed.csv\")\n",
    "sample = pd.read_csv('../data/raw/sample_submission.csv')\n",
    "\n",
    "train = train.dropna(subset='sii').reset_index().drop('index',axis=1)\n",
    "train = train.select_dtypes(include='number')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot learning curves\n",
    "def plot_learning_curve(model, X, y, cv):\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(model, X, y, cv=cv,\n",
    "                                                             scoring=make_scorer(custom_rmse, greater_is_better=False),\n",
    "                                                             train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "                                                             n_jobs=-1)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)  # Convert from negative RMSE to RMSE\n",
    "    valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, label=\"Training RMSE\")\n",
    "    plt.plot(train_sizes, valid_scores_mean, label=\"Cross-Validation RMSE\")\n",
    "    plt.title(f\"Learning Curve ({model.__class__.__name__})\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Optuna optimization functions for each model\n",
    "\n",
    "# CatBoost\n",
    "def objective_catboost(trial, X_train, y_train):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10),\n",
    "        'random_strength': trial.suggest_uniform('random_strength', 1, 20),\n",
    "        'bagging_temperature': trial.suggest_uniform('bagging_temperature', 0, 1),\n",
    "        'border_count': trial.suggest_int('border_count', 1, 255),\n",
    "    }\n",
    "\n",
    "    model = CatBoostRegressor(**params, verbose=0, early_stopping_rounds=50)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, scoring=make_scorer(custom_rmse, greater_is_better=False), cv=cv, n_jobs=-1)\n",
    "\n",
    "    return np.mean(score)  # Optuna tries to minimize, so we negate RMSE\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "def objective_xgboost(trial, X_train, y_train):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10),\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, scoring=make_scorer(custom_rmse, greater_is_better=False), cv=cv, n_jobs=-1)\n",
    "\n",
    "    return np.mean(score)\n",
    "\n",
    "\n",
    "# LightGBM\n",
    "def objective_lightgbm(trial, X_train, y_train):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "    }\n",
    "\n",
    "    model = LGBMRegressor(**params)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, scoring=make_scorer(custom_rmse, greater_is_better=False), cv=cv, n_jobs=-1)\n",
    "\n",
    "    return np.mean(score)\n",
    "\n",
    "\n",
    "# RandomForest\n",
    "def objective_randomforest(trial, X_train, y_train):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**params)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, scoring=make_scorer(custom_rmse, greater_is_better=False), cv=cv, n_jobs=-1)\n",
    "\n",
    "    return np.mean(score)\n",
    "\n",
    "\n",
    "# GradientBoosting\n",
    "def objective_gbr(trial, X_train, y_train):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "    }\n",
    "\n",
    "    model = GradientBoostingRegressor(**params)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, scoring=make_scorer(custom_rmse, greater_is_better=False), cv=cv, n_jobs=-1)\n",
    "\n",
    "    return np.mean(score)\n",
    "\n",
    "# LinearRegression (no hyperparameters to tune)\n",
    "def objective_lr(trial, X_train, y_train):\n",
    "    model = LinearRegression()\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, scoring=make_scorer(custom_rmse, greater_is_better=False), cv=cv, n_jobs=-1)\n",
    "\n",
    "    return np.mean(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run hyperparameter tuning and plot learning curve for all models\n",
    "def tune_model_with_learning_curve(model_name, X_train, y_train):\n",
    "    if model_name == 'CatBoost':\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective_catboost(trial, X_train, y_train), n_trials=50)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "\n",
    "        # Train model with best parameters and plot learning curve\n",
    "        model = CatBoostRegressor(**best_params, verbose=0, early_stopping_rounds=50)\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        plot_learning_curve(model, X_train, y_train, cv)\n",
    "\n",
    "        return best_params\n",
    "\n",
    "    elif model_name == 'XGBoost':\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective_xgboost(trial, X_train, y_train), n_trials=50)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "\n",
    "        model = XGBRegressor(**best_params)\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        plot_learning_curve(model, X_train, y_train, cv)\n",
    "\n",
    "        return best_params\n",
    "\n",
    "    elif model_name == 'LightGBM':\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective_lightgbm(trial, X_train, y_train), n_trials=50)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "\n",
    "        model = LGBMRegressor(**best_params)\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        plot_learning_curve(model, X_train, y_train, cv)\n",
    "\n",
    "        return best_params\n",
    "\n",
    "    elif model_name == 'RandomForest':\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective_randomforest(trial, X_train, y_train), n_trials=50)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "\n",
    "        model = RandomForestRegressor(**best_params)\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        plot_learning_curve(model, X_train, y_train, cv)\n",
    "\n",
    "        return best_params\n",
    "\n",
    "    elif model_name == 'GradientBoosting':\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective_gbr(trial, X_train, y_train), n_trials=50)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "\n",
    "        model = GradientBoostingRegressor(**best_params)\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        plot_learning_curve(model, X_train, y_train, cv)\n",
    "\n",
    "        return best_params\n",
    "\n",
    "    elif model_name == 'LinearRegression':\n",
    "        print(\"No hyperparameters to tune for Linear Regression.\")\n",
    "        model = LinearRegression()\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        plot_learning_curve(model, X_train, y_train, cv)\n",
    "\n",
    "        return \"No hyperparameters tuned for Linear Regression.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
